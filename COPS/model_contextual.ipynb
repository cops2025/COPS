{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from UtilityMethods import utils\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "import pulp as p\n",
    "import math\n",
    "from copy import copy\n",
    "import pprint as pp\n",
    "import itertools\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "# IS_VISIT_DEPENDENT = False # whether the above empirical estimates are visit-dependent or not\n",
    "\n",
    "DATA3= 'xxx/COPS/BPBGClass/data/ACCORD_BPBGClass_v2_merged.csv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/ACCORD_BPBGClass_v2_merged.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.8/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.8/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.8/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.8/site-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.8/site-packages/pandas/io/common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/ACCORD_BPBGClass_v2_merged.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATA3)\n",
    "print(df.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorize the context features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catrgorize the edu_baseline into 3 columns\n",
    "\n",
    "edu1=[]\n",
    "edu2=[]\n",
    "edu3=[]\n",
    "for i in range(df.shape[0]):\n",
    "    row = df.iloc[i]\n",
    "    edu = row['edu_baseline']\n",
    "\n",
    "    if edu == 1:\n",
    "        edu1.append(1)\n",
    "        edu2.append(0)\n",
    "        edu3.append(0)\n",
    "    elif edu == 2:\n",
    "        edu1.append(0)\n",
    "        edu2.append(1)\n",
    "        edu3.append(0)\n",
    "    elif edu == 3:\n",
    "        edu1.append(0)\n",
    "        edu2.append(0)\n",
    "        edu3.append(1)\n",
    "    elif edu == 4:\n",
    "        edu1.append(0)\n",
    "        edu2.append(0)\n",
    "        edu3.append(0)\n",
    "    else:\n",
    "        print('error')\n",
    "        exit()\n",
    "\n",
    "df['edu_baseline_1'] = edu1\n",
    "df['edu_baseline_2'] = edu2\n",
    "df['edu_baseline_3'] = edu3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorize the cigarett_baseline into 0/1, 1  = 1, 2 = 0\n",
    "cig1 = []\n",
    "for i in range(df.shape[0]):\n",
    "    row = df.iloc[i]\n",
    "    cig = row['cigarett_baseline']\n",
    "    if cig == 1:\n",
    "        cig1.append(1)\n",
    "    elif cig == 2:\n",
    "        cig1.append(0)\n",
    "    else:\n",
    "        print('error')\n",
    "        exit()\n",
    "\n",
    "df['cigarett_baseline_1'] = cig1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorize the sbp_discrete_merged (0, 1, 2) into 2 columns, BUT WE WILL NOT USE 2 columns, but just 1 column\n",
    "# sbp1 = []\n",
    "# sbp2 = []\n",
    "# for i in range(df.shape[0]):\n",
    "#     row = df.iloc[i]\n",
    "#     sbp = row['sbp_discrete_merged']\n",
    "#     if sbp == 0:\n",
    "#         sbp1.append(1)\n",
    "#         sbp2.append(0)\n",
    "#     elif sbp == 1:\n",
    "#         sbp1.append(0)\n",
    "#         sbp2.append(1)\n",
    "#     elif sbp == 2:\n",
    "#         sbp1.append(0)\n",
    "#         sbp2.append(0)\n",
    "#     else:\n",
    "#         print('error')\n",
    "#         exit()\n",
    "\n",
    "# df['sbp_discrete_merged_1'] = sbp1\n",
    "# df['sbp_discrete_merged_2'] = sbp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(139005, 81)\n",
      "Index(['MaskID', 'Visit', 'glycemia', 'bp', 'sbp', 'dbp', 'hr', 'hba1c', 'TC',\n",
      "       'trig', 'vldl', 'ldl', 'hdl', 'fpg', 'alt', 'cpk', 'potassium',\n",
      "       'screat', 'gfr', 'ualb', 'ucreat', 'uacr', 'edu_baseline',\n",
      "       'yrsdiab_baseline', 'yrstens_baseline', 'cigarett_baseline',\n",
      "       'wt_kg_baseline', 'ht_cm_baseline', 'wt_kg_visit', 'ht_cm_visit',\n",
      "       'oral_gmed', 'medadd', 'medchg_intbp', 'medchg_stdbp', 'bp_med', 'BMI',\n",
      "       'female', 'baseline_age', 'cvd_hx_baseline', 'raceclass', 'type_po',\n",
      "       'CVDRisk', 'BPClass', 'BGClass', 'sbp_discrete', 'hba1c_discrete',\n",
      "       'BMI_discrete', 'hdl_discrete', 'TC_discrete', 'sbp_feedback',\n",
      "       'hba1c_feedback', 'CVDRisk_feedback', 'bpclass_none', 'Diur', 'ACE',\n",
      "       'Beta-blocker', 'CCB', 'ARB', 'Alpha-Beta-blocker', 'Alpha-blocker',\n",
      "       'Sympath', 'Vasod', 'bgclass_none', 'Bingu', 'Thiaz', 'Sulfon',\n",
      "       'Meglit', 'Alpha-gluc', 'baseline_BMI', 'race_whiteother', 'race_black',\n",
      "       'CVDRisk_feedback_binary', 'BMI_feedback', 'TC_feedback',\n",
      "       'hba1c_discrete_merged', 'action_code', 'state_code', 'edu_baseline_1',\n",
      "       'edu_baseline_2', 'edu_baseline_3', 'cigarett_baseline_1'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State space and action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(states) = 4\n",
      "['00', '01', '10', '11']\n",
      "00 0\n",
      "01 1\n",
      "10 2\n",
      "\n",
      "len(actions) = 16\n",
      "0000 0\n",
      "0001 1\n",
      "0010 2\n",
      "0011 3\n",
      "0100 4\n",
      "Actions for State 0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n"
     ]
    }
   ],
   "source": [
    "# state space, actions available in each state are always the same\n",
    "\n",
    "\"\"\"\n",
    "state_features = ['sbp_discrete','hba1c_discrete','TC_discrete','hdl_discrete','BMI_discrete']\n",
    "sbp_level = ['0', '1', '2', '3'] # possible values for sbp_discrete\n",
    "hba1c_level = ['0', '1', '2', '3', '4', '5', '6', '7']\n",
    "TC_level = ['0', '1', '2', '3']\n",
    "hdl_level = ['0', '1', '2', '3']\n",
    "BMI_level = ['0', '1', '2', '3']\n",
    "\"\"\"\n",
    "\n",
    "# here we merge levels\n",
    "# sbp_level = ['0', '1', '2'] # sbp_discrete, 0: 0, 1:1, 2+3: 2\n",
    "# hba1c_level = ['0', '1', '2'] # hba1c_discrete, 0+1: 0, 2+3: 1, 4+5+6+7: 2\n",
    "\n",
    "sbp_level = ['0', '1',] # sbp_discrete, 0: 0, 1:1, 2+3: 2\n",
    "hba1c_level = ['0', '1', ] # hba1c_discrete, 0+1: 0, 2+3: 1, 4+5+6+7: 2\n",
    "\n",
    "TC_level = ['0', '1'] # TC_discrete, 0+1: 0, 2+3: 1\n",
    "hdl_level = ['0', '1'] # hdl_discrete, 0+1: 0, 2+3: 1\n",
    "\n",
    "# sbp_discrete_code_dict = {'0': '0', '1': '1',\n",
    "#                           '2': '2', '3': '2',}\n",
    "\n",
    "sbp_discrete_code_dict = {'0': '0', '1': '0',\n",
    "                          '2': '1', '3': '1',}\n",
    "\n",
    "# hba1c_discrete_code_dict = {'0': '0', '1': '0',\n",
    "#                             '2': '1', '3': '1',\n",
    "#                             '4': '2', '5': '2',\n",
    "#                             '6': '2', '7': '2'}\n",
    "\n",
    "hba1c_discrete_code_dict = {'0': '0', '1': '0',\n",
    "                            '2': '0', '3': '0',\n",
    "                            '4': '1', '5': '1',\n",
    "                            '6': '1', '7': '1'}\n",
    "\n",
    "TC_discrete_code_dict = {'0': '0', '1': '0',\n",
    "                         '2': '1', '3': '1'}\n",
    "\n",
    "hdl_discrete_code_dict = {'0': '0', '1': '0',\n",
    "                          '2': '1', '3': '1'}\n",
    "\n",
    "# 4 features, state space = 36\n",
    "# state_features = ['sbp_discrete', 'hba1c_discrete', 'TC_discrete', 'hdl_discrete']\n",
    "# combinations = itertools.product(sbp_level, hba1c_level, TC_level, hdl_level)\n",
    "\n",
    "# 3 features, state space = 18\n",
    "# state_features = ['sbp_discrete', 'hba1c_discrete', 'TC_discrete']\n",
    "# combinations = itertools.product(sbp_level, hba1c_level, TC_level)\n",
    "\n",
    "# 2 features, state space = 9\n",
    "combinations = itertools.product(sbp_level, hba1c_level)\n",
    "state_features = ['sbp_discrete', 'hba1c_discrete']\n",
    "\n",
    "# 1 feature, srtate space = 3\n",
    "# combinations = itertools.product(hba1c_level)\n",
    "# state_features = ['hba1c_discrete']\n",
    "\n",
    "states = [''.join(i) for i in combinations]\n",
    "print('len(states) =', len(states))\n",
    "print(states[:5])\n",
    "\n",
    "N_STATES = len(states)\n",
    "state_code_to_index = {code: i for i, code in enumerate(states)}\n",
    "state_index_to_code = {i: code for i, code in enumerate(states)}\n",
    "# print the first 5 state_code_to_index\n",
    "for i in range(3):\n",
    "    print(states[i], state_code_to_index[states[i]])\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "# action space, 000000000 means bgclass_none, 111111111 means all bgmed class are precribed\n",
    "# we donot include 'bgclass_none' as a action, because 000000000 means bgclass_none\n",
    "# action_features = ['Bingu', 'Thiaz', 'Sulfon', 'Meglit'] # pick the top 4 most frequently prescribed bgmed class\n",
    "\n",
    "action_features = ['Diur', 'ACE',\n",
    "                    'Bingu', 'Thiaz', ] # pick the top 2 most frequently prescribed BP and BG Med class\n",
    "\n",
    "combinations = list(itertools.product('01', repeat=len(action_features)))\n",
    "actions = [''.join(i) for i in combinations]\n",
    "print('len(actions) =', len(actions))\n",
    "N_ACTIONS = len(actions) # number of actions = 512\n",
    "action_code_to_index = {code: i for i, code in enumerate(actions)}\n",
    "action_index_to_code = {i: code for i, code in enumerate(actions)}\n",
    "# print the first 5 action_code_to_index\n",
    "for i in range(5):\n",
    "    print(actions[i], action_code_to_index[actions[i]])\n",
    "\n",
    "# build the action space for each state, assign the same action space to all states\n",
    "ACTIONS_PER_STATE = {}\n",
    "for s in range(N_STATES):\n",
    "    ACTIONS_PER_STATE[s] = [i for i in range(N_ACTIONS)] # this is the action code index\n",
    "print('Actions for State 0:', ACTIONS_PER_STATE[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate empirical estimates of P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished adding action_code and state_code columns\n"
     ]
    }
   ],
   "source": [
    "# add the state and action code columns\n",
    "action_code = []\n",
    "state_code = []\n",
    "sbp_discrete_merged = []\n",
    "hba1c_discrete_merged = []\n",
    "for i in range(len(df)):\n",
    "    row = df.iloc[i]\n",
    "    s_code = ''\n",
    "    a_code = ''\n",
    "    for state_fea in state_features:\n",
    "        code = str(row[state_fea])\n",
    "\n",
    "        # merge 3 with 2 for sbp_discrete and TC_discrete\n",
    "        if state_fea == 'sbp_discrete':\n",
    "            code = sbp_discrete_code_dict[code]\n",
    "            sbp_discrete_merged.append(int(code))\n",
    "        elif state_fea == 'hba1c_discrete':\n",
    "            code = hba1c_discrete_code_dict[code]\n",
    "            hba1c_discrete_merged.append(int(code))\n",
    "        elif state_fea == 'TC_discrete':\n",
    "            code = TC_discrete_code_dict[code]\n",
    "        elif state_fea == 'hdl_discrete':\n",
    "            code = hdl_discrete_code_dict[code]\n",
    "        else:\n",
    "            raise ValueError('state_fea not recognized')\n",
    "            exit(1)\n",
    "\n",
    "        s_code += code\n",
    "\n",
    "    for action_fea in action_features:\n",
    "        a_code += str(row[action_fea])\n",
    "\n",
    "    action_code.append(a_code)\n",
    "    state_code.append(s_code)\n",
    "\n",
    "assert len(hba1c_discrete_merged) == len(df)\n",
    "\n",
    "df['sbp_discrete_merged'] = sbp_discrete_merged\n",
    "df['hba1c_discrete_merged'] = hba1c_discrete_merged\n",
    "df['action_code'] = action_code\n",
    "df['state_code'] = state_code\n",
    "print('Finished adding action_code and state_code columns')\n",
    "\n",
    "# DATA_MERGED = DATA[:-4] + '_merged.csv'\n",
    "# # write the merged data to file\n",
    "# df.to_csv(DATA_MERGED, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(DATA3)\n",
    "\n",
    "# save df to csv, replace _merged.csv with _contextual.csv\n",
    "# fn = '../data/ACCORD_BPBGClass_v2_contextual.csv'\n",
    "# df.to_csv(fn, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10251/10251 [00:44<00:00, 231.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(visit_number) = 3595\n",
      "averge visit_number = 38.666203059805284\n",
      "len(count_s_a) = 64\n",
      "len(count_s_a_d) = 256\n",
      "Finished counting by looping through the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#------------- calculate the empirical estimate of P based on entire dataset ----------------\n",
    "\n",
    "count_s_a = {} # count the number of times state s and action a appear in the dataset, sparse format\n",
    "count_s_a_d = {} # count the number of times state s, action a, and next state s' appear in the dataset\n",
    "visit_number = [] # number of visits for each patient\n",
    "\n",
    "# loop through each patient in the dataset\n",
    "for i in tqdm(range(100001, 110252)):\n",
    "    df_patient = df[df['MaskID'] == i]\n",
    "\n",
    "    if len(df_patient) > 0:\n",
    "        visit_number.append(len(df_patient))\n",
    "\n",
    "    # loop through each visit of the patient\n",
    "    for j in range(len(df_patient)-1): # loop before last visit\n",
    "        row = df_patient.iloc[j]\n",
    "        s_code = row['state_code']\n",
    "        a_code = row['action_code']\n",
    "        ns_code = df_patient.iloc[j+1]['state_code']\n",
    "\n",
    "        # convert from code to index\n",
    "        s = state_code_to_index[s_code]\n",
    "        a = action_code_to_index[a_code]\n",
    "        s_ = state_code_to_index[ns_code]\n",
    "\n",
    "        if (s, a) not in count_s_a:\n",
    "            count_s_a[(s, a)] = 1\n",
    "        else:\n",
    "            count_s_a[(s, a)] += 1\n",
    "\n",
    "        if (s, a, s_) not in count_s_a_d:\n",
    "            count_s_a_d[(s, a, s_)] = 1\n",
    "        else:\n",
    "            count_s_a_d[(s, a, s_)] += 1\n",
    "\n",
    "print('len(visit_number) =', len(visit_number))\n",
    "print('averge visit_number =', sum(visit_number)/len(visit_number))\n",
    "\n",
    "print('len(count_s_a) =', len(count_s_a))\n",
    "print('len(count_s_a_d) =', len(count_s_a_d))\n",
    "print('Finished counting by looping through the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total possible state-action pairs = 64\n",
      "Seen state-action pairs = 64\n",
      "Unseen state-action pairs = 0\n",
      "Sparsity of state-action pairs = 0.0\n"
     ]
    }
   ],
   "source": [
    "# calculate the sparsity of state-action pairs\n",
    "print('Total possible state-action pairs =', N_STATES * N_ACTIONS)\n",
    "print('Seen state-action pairs =', len(count_s_a))\n",
    "print('Unseen state-action pairs =', N_STATES * N_ACTIONS - len(count_s_a))\n",
    "print('Sparsity of state-action pairs =', 1 - len(count_s_a)/(N_STATES * N_ACTIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished initializing P\n",
      "Finished calculating the empirical estimate of P\n",
      "\n",
      "Details of P, R, C:\n",
      "P: 100.000000% are non-zeros\n",
      "\n",
      "Sample values of P, R, C:\n",
      "P[0][0][0] = 0.9473557499398122\n"
     ]
    }
   ],
   "source": [
    "# initialize P, NOT using sparse matrix format\n",
    "P = {} # N_STATES * N_ACTIONS * N_STATES, dictionary of transition probability matrices, based on the entire dataset\n",
    "\n",
    "for s in range(N_STATES):\n",
    "    l = len(actions)\n",
    "\n",
    "    P[s] = {}\n",
    "    for a in range(N_ACTIONS):\n",
    "        P[s][a] = np.zeros(N_STATES)\n",
    "\n",
    "print('Finished initializing P')\n",
    "\n",
    "for (s, a, s_) in count_s_a_d:\n",
    "    P[s][a][s_] = count_s_a_d[(s, a, s_)]/max(count_s_a[(s, a)],1)\n",
    "\n",
    "print('Finished calculating the empirical estimate of P')\n",
    "\n",
    "#------------- check the sparsity of P, R, C\n",
    "print('\\nDetails of P, R, C:')\n",
    "print('P: {:.6f}% are non-zeros'.format(len(count_s_a_d)*100/(N_STATES*N_ACTIONS*N_STATES)))\n",
    "\n",
    "# print sample values of P, R, C\n",
    "print('\\nSample values of P, R, C:')\n",
    "print('P[0][0][0] =', P[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the values of the P, sort the values when print out\n",
    "\n",
    "# loop through each values in P, build transition string\n",
    "transition_strings = {}\n",
    "for s in range(N_STATES):\n",
    "    for a in range(N_ACTIONS):\n",
    "        for i in range(N_STATES):\n",
    "            text = f'P[{s}][{a}][{i}]'\n",
    "            value = P[s][a][i]\n",
    "            if value > 0:\n",
    "                transition_strings[text] = value\n",
    "            else:\n",
    "                print(f'P[{s}][{a}][{i}] = 0')\n",
    "                exit()\n",
    "\n",
    "# sort the transition strings based on the values\n",
    "sorted_transition_strings = sorted(transition_strings.items(), key=lambda x: x[1], reverse=True)\n",
    "print('\\nSorted transition strings:')\n",
    "for text, value in sorted_transition_strings:\n",
    "    print(f'{text} = {value:.6f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Init states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(INIT_STATES_LIST) = 4\n",
      "df_blr.shape = (3595, 82)\n",
      "state_code\n",
      "01    1561\n",
      "11    1320\n",
      "00     414\n",
      "10     300\n",
      "Name: state_code, dtype: int64\n",
      "\n",
      "most_freq_blr_state = 01\n",
      "INIT_STATE_INDEX = 1\n"
     ]
    }
   ],
   "source": [
    "def check_frequency(df, col_name):\n",
    "    print(col_name)\n",
    "    df = df[col_name]\n",
    "    df = df.value_counts()\n",
    "    print(df)\n",
    "    print()\n",
    "\n",
    "    # return the first index in the series\n",
    "    return df.index[0]\n",
    "\n",
    "# get the rows when the visit=='BLR' in df\n",
    "df_blr = df[df['Visit']=='BLR']\n",
    "INIT_STATES_LIST = df_blr['state_code'].unique() # we will sample uniformly from this list\n",
    "print('len(INIT_STATES_LIST) =', len(INIT_STATES_LIST))\n",
    "\n",
    "print('df_blr.shape =', df_blr.shape)\n",
    "most_freq_blr_state = check_frequency(df_blr, 'state_code')\n",
    "print('most_freq_blr_state =', most_freq_blr_state)\n",
    "\n",
    "INIT_STATE_INDEX = state_code_to_index[most_freq_blr_state]\n",
    "print('INIT_STATE_INDEX =', INIT_STATE_INDEX)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check frequency of context features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_fea = ['baseline_age', 'female', 'race_whiteother',\n",
    "                'edu_baseline_1',\n",
    "                'edu_baseline_2',\n",
    "                'edu_baseline_3',\n",
    "                'cvd_hx_baseline',\n",
    "                'baseline_BMI',\n",
    "                # 'baseline_BMI_discrete',\n",
    "                # 'cigarett_baseline',\n",
    "                'cigarett_baseline_1',\n",
    "               ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build the CONTEXT_VECTOR_dict \n",
    "\n",
    "each key is the MaskID, value is the corresponding CONTEXT_VECTOR for the patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(mask_id_list) = 3595\n",
      "df_blr.shape = (3595, 82)\n",
      "len(CONTEXT_VECTOR_dict) = 3595\n"
     ]
    }
   ],
   "source": [
    "def get_context_vec(row, context_fea):\n",
    "    context_vec = np.zeros(len(context_fea))\n",
    "    for i in range(len(context_fea)):\n",
    "        context_vec[i] = row[context_fea[i]]\n",
    "    return context_vec\n",
    "\n",
    "def get_value_recorded(df, mask_id, col_name):\n",
    "    df_patient = df[df['MaskID'] == mask_id]\n",
    "    if len(df_patient) > 0:\n",
    "        return df_patient[col_name]\n",
    "    else:\n",
    "        print('error: mask_id not found')\n",
    "        return []\n",
    "\n",
    "\n",
    "# build the CONTEXT_VECTOR_dict, each key is the MaskID, value is the corresponding CONTEXT_VECTOR for the patient\n",
    "CONTEXT_VECTOR_dict = {}\n",
    "\n",
    "# get unique MaskID, 4366 patients\n",
    "mask_id_list = df['MaskID'].unique()\n",
    "print('len(mask_id_list) =', len(mask_id_list))\n",
    "\n",
    "# get the BLR visit only\n",
    "df_blr = df[df['Visit']=='BLR']\n",
    "print('df_blr.shape =', df_blr.shape)\n",
    "\n",
    "# loop through each row of df_blr\n",
    "for r in range(df_blr.shape[0]):\n",
    "    row = df_blr.iloc[r]\n",
    "    mask_id = row['MaskID']\n",
    "    # state_code = row['state_code']\n",
    "    context_vec = get_context_vec(row, context_fea)\n",
    "    CONTEXT_VECTOR_dict[mask_id] = context_vec\n",
    "\n",
    "    state_recorded = get_value_recorded(df, mask_id, 'state_code')\n",
    "    action_recorded = get_value_recorded(df, mask_id, 'action_code')\n",
    "\n",
    "    # for each state in state_recorded, convert it to state_index\n",
    "    state_index_recorded = [state_code_to_index[s] for s in state_recorded]\n",
    "    action_index_recorded = [action_code_to_index[a] for a in action_recorded]\n",
    "\n",
    "    init_state_index = state_index_recorded[0]\n",
    "    CONTEXT_VECTOR_dict[mask_id] = (context_vec, init_state_index, state_index_recorded, action_index_recorded)\n",
    "\n",
    "\n",
    "print('len(CONTEXT_VECTOR_dict) =', len(CONTEXT_VECTOR_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(BPBG_mask_id_list) = 3595\n"
     ]
    }
   ],
   "source": [
    "# save the mask_id in CONTEXT_VECTOR_dict to file for use of BPClass and BGClass\n",
    "BPBG_mask_id_list = list(CONTEXT_VECTOR_dict.keys())\n",
    "print('len(BPBG_mask_id_list) =', len(BPBG_mask_id_list))\n",
    "\n",
    "with open('output_final/BPBG_mask_id_list.pkl', 'wb') as f:\n",
    "    pickle.dump(BPBG_mask_id_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(139005, 82)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the KNN model for selecting clinician' action\n",
    "\n",
    "1. build the KNN model using (context_fea, state_index) in the raw recorded data\n",
    "2. save the fitted KNN model for use in Contextual_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "points[0] = [60.8, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 35.91217711238743, 0.0, 0.0, 1.0]\n",
      "len(points) = 139005\n",
      "labels[:5] = [4, 4, 12, 12, 12]\n",
      "len(labels) = 139005\n",
      "distances = [[2.90088456e-15 7.17376748e-06 1.38392751e-05 2.86536623e-03\n",
      "  5.73065905e-03]]\n",
      "indices = [[     1  70532  14240  44892 113885]]\n",
      "nearest_labels = [4, 14, 6, 6, 6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Sample data - list of vectors representing points\n",
    "# use the following columns of df to build the points, context_fea = ['baseline_age', 'female', 'race_whiteother', 'edu_baseline_1', 'edu_baseline_2', 'edu_baseline_3', 'cvd_hx_baseline', 'baseline_BMI', 'cigarett_baseline_1']\n",
    "\n",
    "df['state_index'] = [state_code_to_index[s] for s in df['state_code']]\n",
    "# fea_cols = context_fea + ['state_index']\n",
    "fea_cols = context_fea + ['sbp_discrete_merged', 'hba1c_discrete_merged']\n",
    "\n",
    "\n",
    "points = df[fea_cols].values.tolist()\n",
    "print('points[0] =', points[0])\n",
    "print('len(points) =', len(points))\n",
    "\n",
    "# Corresponding labels for each point\n",
    "labels = [action_code_to_index[a] for a in df['action_code']]\n",
    "print('labels[:5] =', labels[:5])\n",
    "print('len(labels) =', len(labels))\n",
    "\n",
    "\n",
    "# Vector point for which we want to find the nearest neighbors, sample point\n",
    "vector_point = [60.8,\t0,\t1,\t0,\t0,\t0,\t0,\t35.91217711,\t0,\t1, 1]\n",
    "\n",
    "# Create MinMaxScaler and fit_transform the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "normalized_points = scaler.fit_transform(points)\n",
    "normalized_vector_point = scaler.transform([vector_point])\n",
    "\n",
    "# Number of nearest neighbors to find\n",
    "k = 5\n",
    "\n",
    "# Create NearestNeighbors model and fit the dataset\n",
    "knn = NearestNeighbors(n_neighbors=k)\n",
    "knn.fit(normalized_points)\n",
    "\n",
    "# Find the indices of k-nearest neighbors\n",
    "distances, indices = knn.kneighbors(normalized_vector_point)\n",
    "print('distances =', distances)\n",
    "print('indices =', indices)\n",
    "\n",
    "# Get the labels of the k-nearest neighbors\n",
    "nearest_labels = [labels[i] for i in indices[0]]\n",
    "print('nearest_labels =', nearest_labels)\n",
    "\n",
    "# save the knn model to pickle file\n",
    "with open('output_final/knn_model.pkl', 'wb') as f:\n",
    "    pickle.dump(knn, f)\n",
    "\n",
    "with open('output_final/knn_model_label.pkl', 'wb') as f:\n",
    "    pickle.dump(labels, f)\n",
    "\n",
    "# save the scaler model to pickle file\n",
    "with open('output_final/scaler_model.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODE_LENGTH = 20 # average number of visits per patient\n",
    "# CONSTRAINT_list = [16, 10, 10] # deviation * 20 visits\n",
    "# C_b_list = [8, 5, 5]  # change this if you want different baseline policy.\n",
    "\n",
    "# CONSTRAINT_list = [166, 160, 160,\n",
    "#                    166, 160, 160,\n",
    "#                    166, 160, 160,]\n",
    "\n",
    "CONSTRAINT1_list = [220] * N_STATES # deviation * 20 visits\n",
    "C1_b_list = [40] * N_STATES # constraint for baseline policy\n",
    "\n",
    "CONSTRAINT2_list = [16] * N_STATES # deviation * 20 visits\n",
    "C2_b_list = [8] * N_STATES  # constraint for baseline policy\n",
    "\n",
    "\n",
    "delta = 0.01 # bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTEXT_VEC_LENGTH = 9\n",
      "ACTION_CODE_LENGTH = 4\n"
     ]
    }
   ],
   "source": [
    "# dump the model settings and parameters to a pickle file\n",
    "CONTEXT_VEC_LENGTH = len(context_fea)\n",
    "ACTION_CODE_LENGTH = len(action_index_to_code[0])\n",
    "print('CONTEXT_VEC_LENGTH =', CONTEXT_VEC_LENGTH)\n",
    "print('ACTION_CODE_LENGTH =', ACTION_CODE_LENGTH)\n",
    "\n",
    "with open('output_final/model_contextual_BPBG.pkl', 'wb') as f:\n",
    "    pickle.dump([P, CONTEXT_VEC_LENGTH, ACTION_CODE_LENGTH, CONTEXT_VECTOR_dict, INIT_STATE_INDEX, INIT_STATES_LIST, state_code_to_index, state_index_to_code, action_index_to_code,\n",
    "                CONSTRAINT1_list, C1_b_list, CONSTRAINT2_list, C2_b_list, N_STATES, N_ACTIONS, ACTIONS_PER_STATE, EPISODE_LENGTH, delta], f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9c86494cdbaab790faf1630f8596bee794fd9c939f53713dc51278a7ffca15d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
